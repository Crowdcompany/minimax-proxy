# MiniMax Proxy

Ein Express.js basierter Proxy-Server f√ºr die MiniMax AI API mit vollst√§ndiger CORS-Unterst√ºtzung f√ºr Frontend-Anwendungen.

**üîë WICHTIG:** Dieser Proxy verwendet ein **fest eingestelltes KI-Modell** (`minimax-m2.1`). Clients ben√∂tigen **keinen eigenen MiniMax API-Key**.

## Live-Deployment

**Basis-URL:** `https://mmproxy.ccpn.cc`
**Status:** ‚úÖ Live und funktional

## API-Endpunkt

### POST `/v1/chat/completions`

Proxy-Endpunkt f√ºr MiniMax AI Chat Completions API.

#### Vollst√§ndige URL
```
https://mmproxy.ccpn.cc/v1/chat/completions
```

*(Lokale Entwicklung: PORT-Umgebungsvariable muss gesetzt sein)*

#### Request Headers
```
Content-Type: application/json
```

#### Request Body
```json
{
  "messages": [
    {
      "role": "user",
      "content": "Deine Nachricht hier"
    }
  ]
}
```

**üí° Hinweis:** Der `model` Parameter ist optional und wird ignoriert. Der Proxy verwendet immer das fest konfigurierte Modell `minimax-m2.1`.

#### Response
Standard MiniMax API Response im JSON-Format (OpenAI-kompatibel).

## Frontend-Integration

### JavaScript Fetch Example
```javascript
async function sendMessage(prompt) {
  try {
    const response = await fetch('https://mmproxy.ccpn.cc/v1/chat/completions', {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json'
      },
      body: JSON.stringify({
        messages: [
          {
            role: 'user',
            content: prompt
          }
        ]
      })
    });

    if (!response.ok) {
      throw new Error(`HTTP Error! Status: ${response.status}`);
    }

    const data = await response.json();
    return data;
  } catch (error) {
    console.error('API Request Error:', error);
    throw error;
  }
}

// Verwendung
sendMessage("Hallo, wie geht es dir?")
  .then(response => {
    console.log(response.choices[0].message.content);
  })
  .catch(error => {
    console.error('Error:', error);
  });
```

### Axios Example
```javascript
import axios from 'axios';

const apiClient = axios.create({
  baseURL: 'https://mmproxy.ccpn.cc',
  headers: {
    'Content-Type': 'application/json'
  }
});

async function sendMessage(prompt) {
  try {
    const response = await apiClient.post('/v1/chat/completions', {
      messages: [
        {
          role: 'user',
          content: prompt
        }
      ]
    });

    return response.data;
  } catch (error) {
    console.error('API Request Error:', error);
    throw error;
  }
}
```

## CORS-Unterst√ºtzung

Der Proxy unterst√ºtzt vollst√§ndig CORS f√ºr alle Domains:
- `Access-Control-Allow-Origin: *`
- `Access-Control-Allow-Methods: GET, POST, PUT, DELETE, OPTIONS`
- `Access-Control-Allow-Headers: Content-Type, Authorization, X-Requested-With`

## Features

- ‚úÖ **Kein API-Key erforderlich** - Proxy √ºbernimmt die Authentifizierung
- ‚úÖ **Fest konfiguriertes Modell** - `minimax-m2.1` (Advanced reasoning)
- ‚úÖ **Vollst√§ndige CORS-Unterst√ºtzung** f√ºr Web-Anwendungen
- ‚úÖ **Robuste Fehlerbehandlung** mit strukturierten Antworten
- ‚úÖ **Direkter Proxy** zu MiniMax AI API
- ‚úÖ **Einfache Integration** - nur Messages erforderlich
- ‚úÖ **OpenAI-kompatibel** - gleiche API wie OpenRouter

## KI-Modell

**Fest konfiguriert:** `minimax-m2.1`

- ‚úÖ **Advanced reasoning** - Starke Probleml√∂sungsf√§higkeiten
- ‚úÖ **OpenAI-kompatibel** - Standard Chat Completions Format
- ‚úÖ **Keine Modell-Auswahl n√∂tig** - automatisch verwendet
- ‚ùå **Nicht √§nderbar** √ºber die API

## MiniMax API Besonderheiten

### Unterst√ºtzte Parameter
- `messages` - Chat-Nachrichten (erforderlich)
- `model` - KI-Modell (wird √ºberschrieben)
- `temperature` - Kreativit√§t (0.0-1.0, Standard: 1.0)
- `max_tokens` - Maximale Token-Antwort
- `stream` - Streaming-Antworten (boolean)

### Nicht unterst√ºtzte Parameter
- `presence_penalty` - Wird ignoriert
- `frequency_penalty` - Wird ignoriert
- `logit_bias` - Wird ignoriert
- `n` - Nur Wert 1 unterst√ºtzt
- `function_call` - Deprecated, bitte `tools` verwenden

### Besondere Features
- `reasoning_split: true` - Zeigt Denkprozess der KI an

## Lokale Entwicklung

```bash
# Abh√§ngigkeiten installieren
npm install

# Server starten (PORT-Umgebungsvariable erforderlich)
PORT=3000 node index.js

# Mit Docker
docker build -t minimaxproxy .
docker run -e OPENAI_API_KEY=your_key -e PORT=3000 minimaxproxy
```

## Umgebungsvariablen

- `OPENAI_API_KEY`: Dein MiniMax API-Schl√ºssel (auf dem Server bereits konfiguriert)
- `PORT`: Server-Port (wird von Platform gesetzt, z.B. 80/443 f√ºr HTTP/HTTPS)

## Fehlerbehandlung

Bei Fehlern gibt der Proxy strukturierte JSON-Antworten zur√ºck:

```json
{
  "error": "Beschreibung des Fehlers"
}
```

## Test-Frontend

Im `MiniMaxProxyFrontend` Ordner findest du ein vollst√§ndiges Test-Frontend mit HTML, CSS und JavaScript:

```bash
cd MiniMaxProxyFrontend
python3 -m http.server 8001
# √ñffne http://localhost:8001 im Browser
```

## Support

Bei Problemen oder Fragen erstelle ein Issue im Repository oder kontaktiere den Administrator.

## MiniMax vs. OpenRouter

| Feature | MiniMax Proxy | OpenRouter Proxy |
|---------|---------------|------------------|
| Modell | minimax-m2.1 | z-ai/glm-4.5-air:free |
| API | api.minimax.io | openrouter.ai |
| Kompatibilit√§t | OpenAI-kompatibel | OpenAI-kompatibel |
| Reasoning | Advanced | Standard |
